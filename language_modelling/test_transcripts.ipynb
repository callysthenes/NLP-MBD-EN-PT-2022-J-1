{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this is to be used for the language modelling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting html5lib\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "     ------------------------------------ 112.2/112.2 kB 384.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.9 in c:\\users\\pedro\\appdata\\roaming\\python\\python39\\site-packages (from html5lib) (1.16.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\pedro\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
            "Installing collected packages: html5lib\n",
            "Successfully installed html5lib-1.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install html5lib "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#remove the punctuation\n",
        "def remove_punctuation(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(text)\n",
        "    \n",
        "#remove the stop words\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [w for w in text if not w in stop_words]\n",
        "\n",
        "#lemmatize the words\n",
        "def lemmatize_words(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in text]\n",
        "\n",
        "#stem the words\n",
        "def stem_words(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(w) for w in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#take an html file and convert it to a string taking only window.__data__.transcriptList.push( {text:\n",
        "def html_to_string(file):\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html5lib')\n",
        "        for script in soup(['script', 'style']):\n",
        "            script.extract()    # rip it out\n",
        "        text = soup.get_text()\n",
        "        # break into lines and remove leading and trailing space on each\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\")\"))\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def html_to_string(html_file):\n",
        "    with open(html_file, 'r') as f:\n",
        "        html = f.read()\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "##point to a directory on my computer \n",
        "os.chdir(r'C:\\Users\\pedro\\IE Students\\Term 3 - Data Dynamos - Documents\\General\\Natural Language Processing\\transcripts_zoom\\fran_marketing')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\nMBD-PT - ENE-2022 - J-1 - BIG DATA & ARTIFICIAL INTELLIGENCE IN MARKETING - Zoom \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "html_to_string('session1.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pedro\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No tables found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpedro\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mIE Students\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTerm 3 - Data Dynamos - Documents\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGeneral\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mNatural Language Processing\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtranscripts_zoom\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mfran_marketing\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m#read in the html files\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_html(path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39msession1.html\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39m#convert the html files to strings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(html_to_string)\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:1113\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[0;32m   1109\u001b[0m validate_header_arg(header)\n\u001b[0;32m   1111\u001b[0m io \u001b[39m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1113\u001b[0m \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1114\u001b[0m     flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[0;32m   1115\u001b[0m     io\u001b[39m=\u001b[39;49mio,\n\u001b[0;32m   1116\u001b[0m     match\u001b[39m=\u001b[39;49mmatch,\n\u001b[0;32m   1117\u001b[0m     header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m   1118\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m   1119\u001b[0m     skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m   1120\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m   1121\u001b[0m     thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m   1122\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1123\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1124\u001b[0m     decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m   1125\u001b[0m     converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m   1126\u001b[0m     na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m   1127\u001b[0m     keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m   1128\u001b[0m     displayed_only\u001b[39m=\u001b[39;49mdisplayed_only,\n\u001b[0;32m   1129\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:939\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     \u001b[39massert\u001b[39;00m retained \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m--> 939\u001b[0m     \u001b[39mraise\u001b[39;00m retained\n\u001b[0;32m    941\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[0;32m    942\u001b[0m \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables:\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:919\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m p \u001b[39m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only)\n\u001b[0;32m    918\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 919\u001b[0m     tables \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse_tables()\n\u001b[0;32m    920\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m caught:\n\u001b[0;32m    921\u001b[0m     \u001b[39m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    922\u001b[0m     \u001b[39m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    923\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(io, \u001b[39m\"\u001b[39m\u001b[39mseekable\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m io\u001b[39m.\u001b[39mseekable():\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:239\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_tables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    232\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     tables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_tables(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_doc(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs)\n\u001b[0;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables)\n",
            "File \u001b[1;32mc:\\Users\\pedro\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:569\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[1;34m(self, doc, match, attrs)\u001b[0m\n\u001b[0;32m    566\u001b[0m tables \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mfind_all(element_name, attrs\u001b[39m=\u001b[39mattrs)\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tables:\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo tables found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    571\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    572\u001b[0m unique_tables \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
            "\u001b[1;31mValueError\u001b[0m: No tables found"
          ]
        }
      ],
      "source": [
        "#point to a directory on my computer to get the html files\n",
        "path = r'C:\\Users\\pedro\\IE Students\\Term 3 - Data Dynamos - Documents\\General\\Natural Language Processing\\transcripts_zoom\\fran_marketing'\n",
        "#read in the html files\n",
        "df = pd.read_html(path + 'session1.html')\n",
        "#convert the html files to strings\n",
        "df['transcript'] = df['transcript'].apply(html_to_string)\n",
        "#remove the punctuation\n",
        "df['transcript'] = df['transcript'].apply(remove_punctuation)\n",
        "#remove the stop words\n",
        "df['transcript'] = df['transcript'].apply(remove_stop_words)\n",
        "#lemmatize the words\n",
        "df['transcript'] = df['transcript'].apply(lemmatize_words)\n",
        "#stem the words\n",
        "df['transcript'] = df['transcript'].apply(stem_words)\n",
        "#convert the list of words to a string\n",
        "df['transcript'] = df['transcript'].apply(lambda x: ' '.join(x))\n",
        "#save the dataframe to a csv file\n",
        "df.to_csv(path + 'transcripts_clean.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "language modelling.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "aea149784bd30735ce6bf75b9a5314843328adf2083b944f20623ef50c2ca00d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
