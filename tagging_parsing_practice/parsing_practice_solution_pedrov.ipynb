{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBD-EN-PT-2022-J-1/blob/main/tagging_parsing_practice/parsing_practice_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3oUMLXLlU40I"
      },
      "outputs": [],
      "source": [
        "#repository_name = \"NLP-MBD-EN-PT-2022-J-1\"\n",
        "#repository_url = 'https://github.com/acastellanos-ie/' + repository_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d7mC64KvlwP",
        "outputId": "2466aecf-61c5-4dd2-d9dc-8dd75f774a58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP-MBD-EN-PT-2022-J-1'...\n",
            "Updating files:  33% (1388/4111)\n",
            "Updating files:  34% (1398/4111)\n",
            "Updating files:  35% (1439/4111)\n",
            "Updating files:  36% (1480/4111)\n",
            "Updating files:  37% (1522/4111)\n",
            "Updating files:  38% (1563/4111)\n",
            "Updating files:  39% (1604/4111)\n",
            "Updating files:  40% (1645/4111)\n",
            "Updating files:  41% (1686/4111)\n",
            "Updating files:  42% (1727/4111)\n",
            "Updating files:  43% (1768/4111)\n",
            "Updating files:  44% (1809/4111)\n",
            "Updating files:  45% (1850/4111)\n",
            "Updating files:  46% (1892/4111)\n",
            "Updating files:  47% (1933/4111)\n",
            "Updating files:  48% (1974/4111)\n",
            "Updating files:  49% (2015/4111)\n",
            "Updating files:  50% (2056/4111)\n",
            "Updating files:  51% (2097/4111)\n",
            "Updating files:  52% (2138/4111)\n",
            "Updating files:  53% (2179/4111)\n",
            "Updating files:  54% (2220/4111)\n",
            "Updating files:  55% (2262/4111)\n",
            "Updating files:  56% (2303/4111)\n",
            "Updating files:  57% (2344/4111)\n",
            "Updating files:  58% (2385/4111)\n",
            "Updating files:  59% (2426/4111)\n",
            "Updating files:  60% (2467/4111)\n",
            "Updating files:  61% (2508/4111)\n",
            "Updating files:  61% (2514/4111)\n",
            "Updating files:  62% (2549/4111)\n",
            "Updating files:  63% (2590/4111)\n",
            "Updating files:  64% (2632/4111)\n",
            "Updating files:  65% (2673/4111)\n",
            "Updating files:  66% (2714/4111)\n",
            "Updating files:  67% (2755/4111)\n",
            "Updating files:  68% (2796/4111)\n",
            "Updating files:  69% (2837/4111)\n",
            "Updating files:  70% (2878/4111)\n",
            "Updating files:  71% (2919/4111)\n",
            "Updating files:  72% (2960/4111)\n",
            "Updating files:  73% (3002/4111)\n",
            "Updating files:  74% (3043/4111)\n",
            "Updating files:  75% (3084/4111)\n",
            "Updating files:  76% (3125/4111)\n",
            "Updating files:  77% (3166/4111)\n",
            "Updating files:  78% (3207/4111)\n",
            "Updating files:  79% (3248/4111)\n",
            "Updating files:  80% (3289/4111)\n",
            "Updating files:  81% (3330/4111)\n",
            "Updating files:  82% (3372/4111)\n",
            "Updating files:  83% (3413/4111)\n",
            "Updating files:  84% (3454/4111)\n",
            "Updating files:  85% (3495/4111)\n",
            "Updating files:  86% (3536/4111)\n",
            "Updating files:  86% (3576/4111)\n",
            "Updating files:  87% (3577/4111)\n",
            "Updating files:  88% (3618/4111)\n",
            "Updating files:  89% (3659/4111)\n",
            "Updating files:  90% (3700/4111)\n",
            "Updating files:  91% (3742/4111)\n",
            "Updating files:  92% (3783/4111)\n",
            "Updating files:  93% (3824/4111)\n",
            "Updating files:  94% (3865/4111)\n",
            "Updating files:  95% (3906/4111)\n",
            "Updating files:  96% (3947/4111)\n",
            "Updating files:  97% (3988/4111)\n",
            "Updating files:  98% (4029/4111)\n",
            "Updating files:  99% (4070/4111)\n",
            "Updating files: 100% (4111/4111)\n",
            "Updating files: 100% (4111/4111), done.\n"
          ]
        }
      ],
      "source": [
        "#! git clone $repository_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DIep7l0jvtUB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "#! pip install -Uqqr $repository_name/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZjVb8Mb44ao"
      },
      "source": [
        "# Parsing Practice\n",
        "\n",
        "The purpose of this practical session is to gain more insight into using parsers and grammars and getting familiarized with the tools provided to this end by NLTK.\n",
        "\n",
        "**Our goals are to:**\n",
        "\n",
        "- Understand the process of parsing better by doing some examples by hand.\n",
        "- Gain insight into grammars by writing a grammar of our own.\n",
        "- Experiment with training a grammar from a corpus and evaluating it.\n",
        "\n",
        "**The practical session is organized in four sections:**\n",
        "\n",
        "- The warm-up, where you will use a given toy grammar to generate parse trees and get a chance to compare your intuition with the results provided by the NLTK parsers.\n",
        "- Then, you will need to design and test your own grammar, again, for a small example.\n",
        "- Next, you will automatically extract a grammar from a corpus of annotated sentences.\n",
        "- Finally, you are going to explore ways to improve automatically extracted grammars through error analysis.\n",
        "\n",
        "Before diving in, it would be helpful to have a look at how NLTK handles parsing. At the end of the Notebook, you will find a summary of the most useful commands. For more helpful explanations and examples, feel free to consult the [Chapter 8 of the NLTK Book](https://www.nltk.org/book/ch08.html), along with the chapter extras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL1GLufC44a1"
      },
      "source": [
        "# Exercise 0\n",
        "\n",
        "Have a look at the texts and at the grammar below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls1s2_so44a2"
      },
      "source": [
        "### Input texts:\n",
        "\n",
        "    I saw an elephant\n",
        "    I saw an elephant in my pajamas\n",
        "\n",
        "### Grammar:                \n",
        "\n",
        "    S   -> NP VP     [1.0]\n",
        "    PP  -> P NP      [1.0]\n",
        "    NP  -> Det N     [0.4]\n",
        "    NP  -> Det N PP  [0.2]\n",
        "    VP  -> V NP      [0.5]\n",
        "    VP  -> VP PP     [0.5]\n",
        "\n",
        "    NP  -> 'I'        [0.4]\n",
        "    Det -> 'an'       [0.5]\n",
        "    Det -> 'my'       [0.5]\n",
        "    N   -> 'elephant' [0.5]\n",
        "    N   -> 'pajamas'  [0.5]\n",
        "    V   -> 'saw'      [1.0]\n",
        "    P   -> 'in'       [1.0]\n",
        "\n",
        "\n",
        "\n",
        "1.- Load the previous grammar utilizing the `fromstring` method of the `PCFG` class.\n",
        "\n",
        "2.- Using this grammar, try to parse the sample sentences with the NLTK `ChartParser` and then with the  NLTK `Viterbi` parser. This parser covers the two methodologies that we explained in class. The `ChartParser` does not care about probabilities; it just tries to find all the possible parses (trees) that fit the sentence. In contrast, Viterbi is a Dynamic Programming algorithm that tries to recursively find the best parse (tree) for the sentence based on the probabilities defined by the grammar.\n",
        "\n",
        "3.- Examine the output of the parsers. Is there any difference?\n",
        "\n",
        "4.- Using the NLTK GUI, draw the most probable parse trees found by the NLTK parsers and compare them. \n",
        "\n",
        "How are they different?\n",
        "Do they differ only in the names of the nonterminals' use, or is it a structural difference as well?\n",
        "Why do you think these differences occur?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcQblORL44a2"
      },
      "source": [
        "# Solution 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting click\n",
            "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
            "     ------------------------------------ 267.7/267.7 kB 374.8 kB/s eta 0:00:00\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\pedro\\anaconda3\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q4iyL-S5LR5",
        "outputId": "be87b70a-dbb7-4cab-b389-9ce86253cde7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tree import *\n",
        "from nltk.draw import tree\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-QJ0_Ig44a3",
        "outputId": "d14d335f-4df1-4a26-9080-1e307204e88b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Grammar creation\n",
        "grammar = nltk.PCFG.fromstring(\"\"\"\n",
        "S -> NP VP \t[1.0]\n",
        "PP -> P NP \t[1.0]\n",
        "NP -> Det N \t[0.4]\n",
        "NP -> Det N PP \t[0.2] \n",
        "NP -> 'I' \t[0.4]\n",
        "VP -> V NP \t[0.5]\n",
        "VP -> VP PP \t[0.5]\n",
        "Det -> 'an' \t[0.5] \n",
        "Det -> 'my' \t[0.5]\n",
        "N -> 'elephant' [0.5] \n",
        "N ->  'pajamas' [0.5]\n",
        "V -> 'saw' \t[1.0]\n",
        "P -> 'in' \t[1.0]\n",
        "\"\"\")\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sentences\n",
            "I saw an elephant\n",
            "I saw an elephant in my pajamas\n"
          ]
        }
      ],
      "source": [
        "# Import example sentences to NLTK and tokenize them\n",
        "str_sentence1 = \"I saw an elephant\"\n",
        "str_sentence2 = \"I saw an elephant in my pajamas\"\n",
        " \n",
        "print(\"Example sentences\")\n",
        "print(str_sentence1)\n",
        "print(str_sentence2)\n",
        "tokens1 = nltk.word_tokenize(str_sentence1)\n",
        "tokens2 = nltk.word_tokenize(str_sentence2)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse trees obtained with the Chart parser\n",
            "Sentence 1\n",
            "(S (NP I) (VP (V saw) (NP (Det an) (N elephant))))\n",
            "         S                  \n",
            "  _______|___                \n",
            " |           VP             \n",
            " |    _______|___            \n",
            " |   |           NP         \n",
            " |   |        ___|_____      \n",
            " NP  V      Det        N    \n",
            " |   |       |         |     \n",
            " I  saw      an     elephant\n",
            "\n",
            "Sentence 2\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "     S                                      \n",
            "  ___|_____________                          \n",
            " |                 VP                       \n",
            " |        _________|__________               \n",
            " |       VP                   PP            \n",
            " |    ___|___              ___|___           \n",
            " |   |       NP           |       NP        \n",
            " |   |    ___|_____       |    ___|_____     \n",
            " NP  V  Det        N      P  Det        N   \n",
            " |   |   |         |      |   |         |    \n",
            " I  saw  an     elephant  in  my     pajamas\n",
            "\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
            "     S                                  \n",
            "  ___|_________                          \n",
            " |             VP                       \n",
            " |    _________|______                   \n",
            " |   |                NP                \n",
            " |   |    ____________|___               \n",
            " |   |   |     |          PP            \n",
            " |   |   |     |       ___|___           \n",
            " |   |   |     |      |       NP        \n",
            " |   |   |     |      |    ___|_____     \n",
            " NP  V  Det    N      P  Det        N   \n",
            " |   |   |     |      |   |         |    \n",
            " I  saw  an elephant  in  my     pajamas\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the Chart and Viterbi parsers, with the input grammar \n",
        "chart_parser = nltk.ChartParser(grammar)\n",
        "viterbi_parser = nltk.ViterbiParser(grammar)\n",
        " \n",
        "# Results for the Chart Parser\n",
        "print(\"Parse trees obtained with the Chart parser\")\n",
        "print(\"Sentence 1\")\n",
        "for tree in chart_parser.parse(tokens1):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "\n",
        "print(\"Sentence 2\")\n",
        "for tree in chart_parser.parse(tokens2):\n",
        "    print(tree)\n",
        "    tree.pretty_print() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse trees obtained with the Viterbi parser\n",
            "Sentence 1\n",
            "(S (NP I) (VP (V saw) (NP (Det an) (N elephant)))) (p=0.02)\n",
            "         S                  \n",
            "  _______|___                \n",
            " |           VP             \n",
            " |    _______|___            \n",
            " |   |           NP         \n",
            " |   |        ___|_____      \n",
            " NP  V      Det        N    \n",
            " |   |       |         |     \n",
            " I  saw      an     elephant\n",
            "\n",
            "Sentence 2\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP\n",
            "      (Det an)\n",
            "      (N elephant)\n",
            "      (PP (P in) (NP (Det my) (N pajamas)))))) (p=0.001)\n",
            "     S                                  \n",
            "  ___|_________                          \n",
            " |             VP                       \n",
            " |    _________|______                   \n",
            " |   |                NP                \n",
            " |   |    ____________|___               \n",
            " |   |   |     |          PP            \n",
            " |   |   |     |       ___|___           \n",
            " |   |   |     |      |       NP        \n",
            " |   |   |     |      |    ___|_____     \n",
            " NP  V  Det    N      P  Det        N   \n",
            " |   |   |     |      |   |         |    \n",
            " I  saw  an elephant  in  my     pajamas\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Results for the Viterbi Parser\n",
        "print(\"Parse trees obtained with the Viterbi parser\")\n",
        "print(\"Sentence 1\")\n",
        "for tree in viterbi_parser.parse(tokens1):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "\n",
        "print(\"Sentence 2\")\n",
        "for tree in viterbi_parser.parse(tokens2):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Difference between Chart Parser and Viterby parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9t_Kml344a4"
      },
      "source": [
        "We can notice that for the first sentence, the two parsers agree on the result. This is due to the more simple structure of the sentence. For the second sentence, the Chart parser finds two parse trees, while the Viterbi parser outputs only one result. This occurs because the Viterbi parser only computes the most probable parse of the sentences, while the Chart parser is not concerned with the probabilities and it searches for all parse possibilities. In this case, we can notice that the most probable parse for the second sentence is the second parse produced by the Chart parser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_50mYm344a5"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWeDXspE44a5"
      },
      "source": [
        "You are given  the following sentences:\n",
        "\n",
        "    Alice wondered with Bob in the empty city streets\n",
        "    Bob offered Alice an iguana for her birthday\n",
        "    Alice gave an inspiring speech at the conference on education\n",
        "\n",
        "1. Design a non-probabilistic context-free grammar for parsing these sentences and then manually provide the parse tree(s) for these sentences. Here is a list of lexical rules and syntactic categories that you could use to devise the grammar:\n",
        "    Lexical rules:\n",
        "\n",
        "        V    -> 'wondered' | 'offered' | 'gave'\n",
        "        NP   -> 'Alice' | 'Bob'\n",
        "        N    -> 'city' | 'streets' | 'iguana' | 'birthday' | 'speech' | 'conference' | 'education'\n",
        "        Adj  -> 'empty' | 'inspiring'\n",
        "        Det  -> 'the' | 'an' | 'her'\n",
        "        Prep -> 'with' | 'in' | 'for' | 'at' | 'on'\n",
        "        Suggested syntactic categories (non-terminals):\n",
        "\n",
        "        S - Sentence \n",
        "        NP - Noun Phrase \n",
        "        VP - Verb Phrase \n",
        "        PNP - Prepositional Noun Phrase\n",
        "2. With the grammar you designed in point (1.), parse the above sentences using NLTK’s Chart parser, described in Section 1.\n",
        "3. Display and compare the parse trees obtained in points (1.) and (2.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaMstAW44a6"
      },
      "source": [
        "# Solution 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGqBRrK44a6"
      },
      "source": [
        "**1. Design a non-probabilistic context-free grammar for parsing these sentences and then manually provide the parse tree(s) for these sentences. Here is a list of lexical rules and syntactic categories that you could use to devise the grammar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fKIeGm_D44a6"
      },
      "outputs": [],
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP | VP NP PNP | VP PNP | V\n",
        "PNP -> Prep NP | Prep N\n",
        "NP -> Det N | Det N PNP | Det NP | Adj N | Adj NP | N N\n",
        "V -> 'wondered' | 'offered' | 'gave'\n",
        "NP -> 'Alice' | 'Bob'\n",
        "N -> 'city' | 'streets' | 'iguana' | 'birthday' | 'speech' | 'conference' | 'education'\n",
        "Adj -> 'empty' | 'inspiring'\n",
        "Det -> 'the' | 'an' | 'her'\n",
        "Prep -> 'with' | 'in' | 'for' | 'at' | 'on'\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v8TMIrg44a6"
      },
      "source": [
        "**2. With the grammar you designed in point (1.), parse the above sentences using NLTK’s Chart parser, described in Section 1.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T9I1QD-i44a6"
      },
      "outputs": [],
      "source": [
        "str_sentence1 = \"Alice wondered with Bob in the empty city streets\"\n",
        "str_sentence2 = \"Bob offered Alice an iguana for her birthday\"\n",
        "str_sentence3 = \"Alice gave an inspiring speech at the conference on education\"\n",
        "tokens1 = nltk.word_tokenize(str_sentence1)\n",
        "tokens2 = nltk.word_tokenize(str_sentence2)\n",
        "tokens3 = nltk.word_tokenize(str_sentence3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5bkMLAn44a7"
      },
      "source": [
        "**3. Display and compare the parse trees obtained in points (1.) and (2.).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TB6rgA-44a7",
        "outputId": "bfce2299-b185-4448-a4e7-d752ad78fdea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1\n",
            "(S\n",
            "  (NP Alice)\n",
            "  (VP\n",
            "    (VP (VP (V wondered)) (PNP (Prep with) (NP Bob)))\n",
            "    (PNP\n",
            "      (Prep in)\n",
            "      (NP (Det the) (NP (Adj empty) (NP (N city) (N streets)))))))\n",
            "\n",
            "Sentence 2\n",
            "(S\n",
            "  (NP Bob)\n",
            "  (VP\n",
            "    (VP (V offered) (NP Alice))\n",
            "    (NP (Det an) (N iguana))\n",
            "    (PNP (Prep for) (NP (Det her) (N birthday)))))\n",
            "\n",
            "Sentence 3\n",
            "(S\n",
            "  (NP Alice)\n",
            "  (VP\n",
            "    (VP\n",
            "      (VP (V gave) (NP (Det an) (NP (Adj inspiring) (N speech))))\n",
            "      (PNP (Prep at) (NP (Det the) (N conference))))\n",
            "    (PNP (Prep on) (N education))))\n",
            "(S\n",
            "  (NP Alice)\n",
            "  (VP\n",
            "    (VP\n",
            "      (VP (V gave))\n",
            "      (NP (Det an) (NP (Adj inspiring) (N speech)))\n",
            "      (PNP (Prep at) (NP (Det the) (N conference))))\n",
            "    (PNP (Prep on) (N education))))\n",
            "(S\n",
            "  (NP Alice)\n",
            "  (VP\n",
            "    (VP (V gave) (NP (Det an) (NP (Adj inspiring) (N speech))))\n",
            "    (PNP\n",
            "      (Prep at)\n",
            "      (NP (Det the) (N conference) (PNP (Prep on) (N education))))))\n",
            "(S\n",
            "  (NP Alice)\n",
            "  (VP\n",
            "    (VP (V gave))\n",
            "    (NP (Det an) (NP (Adj inspiring) (N speech)))\n",
            "    (PNP\n",
            "      (Prep at)\n",
            "      (NP (Det the) (N conference) (PNP (Prep on) (N education))))))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chart_parser = nltk.ChartParser(grammar)\n",
        "\n",
        "print(\"Sentence 1\")\n",
        "for tree in chart_parser.parse(tokens1):\n",
        "    print(tree)\n",
        "print()\n",
        "\n",
        "print(\"Sentence 2\")\n",
        "for tree in chart_parser.parse(tokens2):\n",
        "    print(tree)\n",
        "print()\n",
        "    \n",
        "print(\"Sentence 3\")\n",
        "for tree in chart_parser.parse(tokens3):\n",
        "    print(tree)\n",
        "print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PICKUP FROM HERE!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmxe1xih44a7"
      },
      "source": [
        "# Exercise 2: Extracting a Grammar from a Corpus\n",
        "\n",
        "We have designed very simple grammars in the previous exercises. Even for these minimal examples, designing grammars are time-consuming and does not necessarily have the best results. \n",
        "\n",
        "We explore a different approach to creating grammars in this exercise: **automatic extraction from annotated texts**.\n",
        "\n",
        "We start from an annotated text: the `Treebank corpus` from the NLTK framework, which includes a small subset (5% fragment) of the `Penn Treebank Corpus`. It contains data from `Wall Street Journal` for 1650 sentences:\n",
        "\n",
        "    ( (S\n",
        "        (NP-SBJ\n",
        "          (NP (NNP Pierre) (NNP Vinken) )\n",
        "          (, ,)\n",
        "          (ADJP\n",
        "            (NP (CD 61) (NNS years) )\n",
        "            (JJ old) )\n",
        "          (, ,) )\n",
        "        (VP (MD will)\n",
        "          (VP (VB join)\n",
        "            (NP (DT the) (NN board) )\n",
        "            (PP-CLR (IN as)\n",
        "              (NP (DT a) (JJ nonexecutive) (NN director) ))\n",
        "            (NP-TMP (NNP Nov.) (CD 29) )))\n",
        "        (. .) ))\n",
        "        \n",
        "     \n",
        "The goal is to automatically extract the CFG from the treebank and then evaluate and improve the extracted grammar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_moOeSQ44a8"
      },
      "source": [
        "# Solution 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPf-Q7Zt44a8"
      },
      "source": [
        "#1. First, split the corpus into two parts : one for learning the grammar (training set) and one for evaluating it (test set). For the sake of saving time during this practical session, use 3% of the total number of sentences for the test set. Here is how you could make the split.**\n",
        "\n",
        "        dataset_size = len(treebank.parsed_sents())\n",
        "\n",
        "        ## here, we define the split percentage for the training set and the\n",
        "        ## learning set, in our case ~3% and ~97%\n",
        "\n",
        "        split_size = int(dataset_size * 0.97)\n",
        "        learning_set = treebank.parsed_sents()[:split_size]\n",
        "        test_set = treebank.parsed_sents()[split_size:]\n",
        "\n",
        "   Note that `test_set`  already contains the parses of the sentences. This is what we are going to compare against, in order to assess the quality of the extracted grammar. In addition, we need the raw (not parsed)  format of the sentences in the test set. These raw sentences will be parsed with the extracted grammar and then compared against the reference, stored in test_set.\n",
        "\n",
        "        # create a set containing the raw sentences\n",
        "        sents = treebank.sents()\n",
        "        raw_test_set = [ [ w for w in sents[i] ] for i in range(split_size, dataset_size) ]\n",
        "    \n",
        "**Extract the grammar out of the learning corpus** At this point, it might be useful to have another look at the commands provided in the end of the document. Here is a suggestion on how to do it. Note that we still need the entire lexicon (i.e. rules of the type POS -> word) even though the syntactic rules are extracted from only a portion of the treebank. Assume that you have already separated the learning set from the test set and that the learning set is stored in the  learning_set  variable.\n",
        "\n",
        "        # This is where we will store all of the productions necessary to\n",
        "        # construct the PCFG.\n",
        "        tbank_productions = []\n",
        "\n",
        "        # For all of the (parsed) sentences in the learning set, extract the\n",
        "        # productions (i.e. extract the rules).\n",
        "        for sent in learning_set:\n",
        "            for production in sent.productions():\n",
        "                tbank_productions.append(production)\n",
        "\n",
        "        # Now, we will add the lexical rules for the ENTIRE lexicon.\n",
        "        for word, tag in treebank.tagged_words():\n",
        "\n",
        "            # for each tagged word, we create a tree containing that\n",
        "            # lexical rule, in order to be able to add it to our\n",
        "            # production set tbank_productions.\n",
        "\n",
        "            t = Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
        "            for production in t.productions():\n",
        "                tbank_productions.append(production)\n",
        "\n",
        "        # At this point, we have the syntactic rules extracted from the\n",
        "        # learning set and all of the lexical rules. We are ready to extract\n",
        "        # the PCFG.\n",
        "        tbank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), tbank_productions)\n",
        "        \n",
        "Try to have a look at the grammar. How many rules are there? Imagine you would have to correct it, for instance. You certainly will come to the conclusion that this is not feasible. Indeed, in such a framework (automated extraction) the grammars should not  be modified by hand (maybe not even read), but only the corpus has to be changed/corrected/extended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SChJ6DC444a_"
      },
      "source": [
        "Before executing the code you need to install the pre-trained parsing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6eot7nl44bC",
        "outputId": "04693b75-f804-4448-f5e0-4f0e17ee5eef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnzgEtcW44bD",
        "outputId": "627508b4-d147-4598-a14a-ad940c588573",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting the Data Set\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.grammar import Nonterminal\n",
        "from nltk import Tree\n",
        " \n",
        "print(\"Splitting the Data Set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Separating Learning Set\n",
            "Learning set size: 3796\n",
            "Separating Test Set\n",
            "Test set size: 118\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The size of the learning set has to be chosen so that we avoid\n",
        "# running out of memory when parsing (caused by a too large learned\n",
        "# grammar). However, a too small size of the learning set, could lead\n",
        "# to a grammar that is not useful (i.e. it cannot parse sentences\n",
        "# other than in the learning set).\n",
        " \n",
        "dataset_size = len(treebank.parsed_sents())\n",
        "split_size = int(dataset_size*0.97)\n",
        " \n",
        "print(\"Separating Learning Set\")\n",
        "print(\"Learning set size: \" + str(split_size))\n",
        "learning_set = treebank.parsed_sents()[:split_size]\n",
        " \n",
        "print(\"Separating Test Set\")\n",
        "print(\"Test set size: \" + str(dataset_size - split_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set containing the raw sentences, to be parsed with the extracted grammar\n",
            "Extract the syntactic (and part of the lexical) rules from the learning set\n",
            "173810\n",
            "Extract the rest of the lexical rules from the lexicon\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# This set already contains the parses of the sentences in the test\n",
        "# set. It is what we are going to compare against, in order to assess\n",
        "# the quality of the extracted grammar.\n",
        "test_set = treebank.parsed_sents()[split_size:]\n",
        " \n",
        "print(\"Set containing the raw sentences, to be parsed with the extracted grammar\")\n",
        "sents = treebank.sents()\n",
        "raw_test_set = [ [ w for w in sents[i] ] for i in range(split_size, dataset_size) ]\n",
        " \n",
        "print(\"Extract the syntactic (and part of the lexical) rules from the learning set\")\n",
        " \n",
        "tbank_productions = []\n",
        "for sent in learning_set:\n",
        "    for production in sent.productions():\n",
        "        tbank_productions.append(production)\n",
        "        \n",
        "print(len(tbank_productions))\n",
        "print(\"Extract the rest of the lexical rules from the lexicon\")\n",
        " \n",
        "for word, tag in treebank.tagged_words():\n",
        "    t = Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
        "    for production in t.productions():\n",
        "        tbank_productions.append(production)\n",
        "        \n",
        "print(len(tbank_productions))\n",
        " \n",
        "print(\"Creating grammar\")\n",
        "tbank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), tbank_productions)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Have a look at the grammar:\n",
        "print(tbank_grammar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYwdNGT544bE"
      },
      "source": [
        "**2. Evaluate the performance of the grammars on the test set (the 3% of the sentences from the treebank that you isolated in the beginning of the exercise). What proportion of sentences from the test set have been parsed as in the reference (i.e. are correctly parsed) by the extracted grammar?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7L5ODr_44bE",
        "outputId": "05c00644-a19b-4277-c318-25997513adda"
      },
      "source": [
        "# From the created gramar, you create a parser to annotate new sentences\n",
        "print(\"Initializing parser\")\n",
        "parser = nltk.ViterbiParser(tbank_grammar) # http://www.nltk.org/_modules/nltk/parse/viterbi.html\n",
        "\n",
        "\n",
        "# Test the extracted grammar with the Viterbi parser on one sentence.\n",
        "# The Viterbi parser gives the most probable parse tree\n",
        "# Test all sentences in the test-set and compare them to the reference parsing\n",
        " \n",
        "parse_success = 0;\n",
        "# for i in range(0, len(raw_test_set)):\n",
        "for i in range(0, 3):\n",
        "    print(\"==== Parsing sentence \" + str(i))\n",
        "    test_sent = raw_test_set[i]\n",
        "    # This will raise an exception if the tokens in the test_sentence\n",
        "    # are not covered by the grammar; should not happen.\n",
        "    tbank_grammar.check_coverage(test_sent)\n",
        "    print(test_sent)\n",
        "    print(\"[\" + str(i) + \"] Reference parse:\")\n",
        "    print(test_set[i])\n",
        "    print(\"[\" + str(i) + \"] Parse trees:\")\n",
        "    for tree in parser.parse(test_sent):\n",
        "        print(tree)\n",
        "        print(test_sent[i] == tree)\n",
        "        if test_sent[i] == tree:\n",
        "            ++parse_success\n",
        "        print(parse_success)\n",
        "        \n",
        "print(\"Successfully parsed sentences: \" + str(parse_success) + \" out of \" + str(3))\n",
        "#print(\"Successfully parsed sentences: \" + str(parse_success) + \" out of \" + str(len(test_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwNnaVMr44bE"
      },
      "source": [
        "**3. We now want to study the size of the grammar with respect to the size of the learning set. Split the former learning set (i.e. the 95%  of the treebank) into two parts: one kept for actual learning and another part which is not used. Do this for several percentages of the learning set (10%, 50%, 70%, 80%, 90%, 100%), several times for each ratio. H ave a look at the resulting grammars and try to understand their differences.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIE24v9d44bE"
      },
      "source": [
        "### Useful NLTK commands\n",
        "\n",
        "Grammar creation from a given string:\n",
        "\n",
        "    g = nltk.CFG.fromstring(\"\"\"<grammar rules here>\"\"\")\n",
        "\n",
        "Tokenize a sentence:\n",
        "\n",
        "    tokens = nltk.word_tokenize(\"<sentence>\")\n",
        "    \n",
        "Create different types of parsers starting from a grammar:\n",
        "\n",
        "    chart_parser   = nltk.ChartParser(g)\n",
        "    viterbi_parser = nltk.ViterbiParser(g)\n",
        "\n",
        "Parse a tokenized text:\n",
        "\n",
        "    trees = viterbi_parser.parse(tokens) #for example, for the Viterbi parser\n",
        "\n",
        "Display the parse trees: \n",
        "\n",
        "[ If not done yet, you first need to install this library: \n",
        "http://tkinter.unpythonic.net/wiki/How_to_install_Tkinter \n",
        "]\n",
        "\n",
        "The command below will generate a popup window, containing the graphical representation of the tree; while the parse tree drawing window is open, your script will be paused. The script will be resumed when you close the window.\n",
        "\n",
        "    from nltk.tree import *\n",
        "    from nltk.draw import tree\n",
        "    tree.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIG6COTb44bF"
      },
      "source": [
        "# Additional Materials: Practical Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeHqJctF44bF"
      },
      "source": [
        "NLTK provides us an excellent way to create our own grammars and train them based on an annotated dataset.\n",
        "\n",
        "Nevertheless, as happens with Tagging, most of the time the pre-train modules are enough to solve most of our situation.\n",
        "\n",
        "My recommendation is to use either [`spacy`](https://spacy.io/) or [`Stanford CoreNLP`](https://stanfordnlp.github.io/CoreNLP/) instead of NLTK for that. The parsers of NLTK are slow and outdated, which made them unsuitable for practical situations.\n",
        "\n",
        "In the following, you have an example of usage of Spacy for dependency parsing, applied to specific tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh3exPsV44bF"
      },
      "source": [
        "Firstly, you need to load `spacy` with the desired language, English in this case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wRP-Zxk44bF",
        "outputId": "7d555b7d-b518-4688-9a57-afcba2f57ed1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Import the English model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Import the English model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B8E60lC44bF"
      },
      "source": [
        "Now we can use the spacy object (`nlp`) to perform the desired language annotations to our sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bfOExpt44bF"
      },
      "outputs": [],
      "source": [
        "sent = \"I shot an elephant\"\n",
        "doc=nlp(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uYdsgoV44bF"
      },
      "source": [
        "The doc object now contains the text plus all the annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTW-FIDo44bG",
        "outputId": "5756baa0-34a9-4f19-f796-bc8a1fffadda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['I', 'shot', 'an', 'elephant']\n",
            "Lemmas: ['-PRON-', 'shoot', 'an', 'elephant']\n",
            "POS Tagging: ['PRON', 'VERB', 'DET', 'NOUN']\n",
            "Parsing: ['nsubj', 'ROOT', 'det', 'dobj']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens:\", [t.text for t in doc])\n",
        "print(\"Lemmas:\", [t.lemma_ for t in doc])\n",
        "print(\"POS Tagging:\", [t.pos_ for t in doc])\n",
        "print(\"Parsing:\", [t.dep_ for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4mDIvCS44bG"
      },
      "source": [
        "We could use the dependency parsing annotations to identify different parts of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCTAFaqU44bG",
        "outputId": "d91b2161-a9d4-493d-c56c-d73d43c2ad53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: [I]\n",
            "Verb: [shot]\n",
            "Object: [elephant]\n"
          ]
        }
      ],
      "source": [
        "print(\"Subject:\", [tok for tok in doc if (tok.dep_ == \"nsubj\") ])\n",
        "print(\"Verb:\", [tok for tok in doc if (tok.dep_ == \"ROOT\") ])\n",
        "print(\"Object:\", [tok for tok in doc if (\"obj\" in tok.dep_ ) ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHPuxKKA44bG"
      },
      "source": [
        "Let's use it to see what are the objects preferred by Truman in his speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYz9T1xp44bG",
        "outputId": "ba5a2f0d-1412-427e-f4d1-4e2d4397ea04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('year', 188),\n",
              " ('dollar', 156),\n",
              " ('-PRON-', 101),\n",
              " ('program', 80),\n",
              " ('war', 77),\n",
              " ('which', 64),\n",
              " ('expenditure', 50),\n",
              " ('time', 45),\n",
              " ('government', 43),\n",
              " ('Congress', 35),\n",
              " ('production', 35),\n",
              " ('States', 34),\n",
              " ('period', 33),\n",
              " ('level', 32),\n",
              " ('people', 32),\n",
              " ('economy', 31),\n",
              " ('policy', 29),\n",
              " ('Government', 28),\n",
              " ('service', 28),\n",
              " ('legislation', 28),\n",
              " ('country', 26),\n",
              " ('month', 26),\n",
              " ('force', 25),\n",
              " ('world', 25),\n",
              " ('price', 25),\n",
              " ('increase', 24),\n",
              " ('opportunity', 23),\n",
              " ('Administration', 23),\n",
              " ('appropriation', 23),\n",
              " ('control', 22),\n",
              " ('business', 22),\n",
              " ('peace', 22),\n",
              " ('income', 22),\n",
              " ('responsibility', 21),\n",
              " ('purpose', 21),\n",
              " ('June', 21),\n",
              " ('rate', 21),\n",
              " ('power', 20),\n",
              " ('system', 19),\n",
              " ('future', 19),\n",
              " ('end', 19),\n",
              " ('development', 19),\n",
              " ('September', 19),\n",
              " ('Act', 19),\n",
              " ('worker', 19),\n",
              " ('activity', 18),\n",
              " ('objective', 18),\n",
              " ('nation', 18),\n",
              " ('veteran', 18),\n",
              " ('addition', 18)]"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('state_union')\n",
        "\n",
        "text = state_union.raw(\"1946-Truman.txt\")\n",
        "\n",
        "def detect_most_common_objects(corpus_text, n_verbs = 10):\n",
        "    verbs = []\n",
        "    for sentence in nltk.sent_tokenize(corpus_text):\n",
        "        doc = nlp(sentence)\n",
        "        for tok in doc:\n",
        "            if (\"obj\" in tok.dep_):\n",
        "                verbs.append(tok.lemma_)\n",
        "    return Counter(verbs).most_common(n_verbs)\n",
        "\n",
        "detect_most_common_objects(text, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct3TXwTD5pPk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "parsing_practice_solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f955d8c6e137d82557673c1af8b287ea837227932fd50ff60964f099d496460b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
